# ğŸ“„ PDF Summarizer using LangChain, FAISS, HuggingFace Embeddings, and OpenAI

# ğŸš€ Overview

This project is a **PDF Summarization Application** built with:

* **Streamlit** for the user interface,
* **PyPDF** for PDF text extraction,
* **LangChain** components for text splitting, embeddings, retrieval, and LLM chaining,
* **FAISS** for vector-based semantic search,
* **OpenAI GPT (gpt-3.5-turbo-16k)** for generating concise summaries.

The app allows a user to upload any PDF file and receive a clear, concise summary generated using a Retrieval-Augmented Generation (RAG) workflow.

---

# âœ¨ Features

* Upload any PDF file through the Streamlit UI.
* Extracts text from all pages using `pypdf`.
* Splits text into chunks for better embedding + retrieval.
* Generates embeddings using `sentence-transformers/all-MiniLM-L6-v2`.
* Stores chunks in a **FAISS vector database**.
* Runs a **retrieval + LLM chain** to produce an accurate summary.
* No hallucinations â€” summary is grounded in retrieved content.

---

# ğŸ§  How It Works

Below is the flow of the entire summarization pipeline:

```
User Uploads PDF
        â†“
Extract Text Using PyPDF
        â†“
Split Text Into Chunks (LangChain TextSplitter)
        â†“
Generate Embeddings (HuggingFaceEmbeddings)
        â†“
Store Vectors in FAISS Vector Database
        â†“
Retriever Fetches Most Relevant Chunks (k=5)
        â†“
LLM (GPT-3.5-Turbo-16k) Summarizes Retrieved Content
        â†“
Return Final Summary to Streamlit UI
```

A clean, structured RAG pipeline designed specifically for summarization.

---

# ğŸ—ï¸ Architecture Diagram (Text Version)

```
                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                  â”‚  Streamlit UI     â”‚
                  â”‚  (Upload PDF)     â”‚
                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
                   Extract Text (pypdf)
                            â†“
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  Text Splitter (1000 tokens) â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
                Generate Embeddings
        (sentence-transformers / MiniLM-L6-v2)
                              â†“
                        FAISS Index
                 (Stores dense vectors)
                              â†“
                  Retriever (Top-k = 5)
                              â†“
                  OpenAI GPT LLM Chain
             (RAG-based Summarization)
                              â†“
                  Streamlit Summary Output
```

---

# ğŸ” Technical Breakdown

A deeper explanation of each major component.

## 1. PDF Text Extraction

Library: **pypdf**

* Reads PDF binary stream.
* Extracts text page by page.
* Concatenates pages into a single raw text string.

Snippet from the project:  îˆ€fileciteîˆ‚turn2file2îˆ

```python
reader = PdfReader(io.BytesIO(pdf_bytes))
full_text = ""
for page in reader.pages:
    full_text += page.extract_text() or ""
```

---

## 2. Text Splitting (Chunking)

Library: **langchain_text_splitters**

* Splits large text into overlapping segments.
* Chunk size: 1000 characters.
* Overlap: 200 characters.

Why?

* LLMs and retrieval work better with smaller, coherent chunks.

Snippet:

```python
splitter = CharacterTextSplitter(
    separator="\n",
    chunk_size=1000,
    chunk_overlap=200,
    length_function=len
)
chunks = splitter.split_text(text)
```

---

## 3. Embedding Generation

Library: **HuggingFaceEmbeddings** using `sentence-transformers/all-MiniLM-L6-v2`.

* Converts text chunks into 384-dimensional dense vectors.
* Captures semantic meaning rather than keywords.

Why MiniLM?

* Lightweight
* Fast
* High accuracy for semantic similarity

Snippet:

```python
embeddings = HuggingFaceEmbeddings(
    model_name="sentence-transformers/all-MiniLM-L6-v2"
)
```

---

## 4. Vector Store â€” FAISS (Local Vector Database)

Library: **faiss-cpu**

* Stores all chunk embeddings.
* Supports efficient similarity search.
* Ideal for local RAG applications.

Snippet:

```python
vector_store = FAISS.from_texts(chunks, embeddings)
```

Retriever used:

```python
retriever = kb.as_retriever(search_kwargs={"k": 5})
```

---

## 5. LLM Summarization (RAG)

Library: **langchain_openai**
Model used: **gpt-3.5-turbo-16k**

* Handles long documents.
* Produces accurate and concise summaries.

Snippet:

```python
llm = ChatOpenAI(model="gpt-3.5-turbo-16k", temperature=0.6)
```

---

## 6. RetrievalQA Chain

Library: **langchain_classic.chains.retrieval_qa.base**

* Combines retriever + LLM into one chain.
* Ensures LLM sees only top-k relevant chunks.
* Prevents hallucinations.

Snippet:

```python
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=retriever,
    return_source_documents=False
)
result = qa_chain.run(query)
```

---

# ğŸ›ï¸ Streamlit Interface

Main UI file: **test.py**  îˆ€fileciteîˆ‚turn2file1îˆ

* Upload widget for PDF files.
* Button triggers summarization.
* Displays the final summary cleanly.

Snippet:

```python
pdf_file = st.file_uploader("Upload your PDF file", type=["pdf"])
submit = st.button("Summarize")
...
st.write(summary)
```

---

# â–¶ï¸ How to Run the Project

```
pip install -r requirements.txt
```

Create a `.env` file:

```
OPENAI_API_KEY=your_api_key_here
```

Run the app:

```
streamlit run test.py
```

---

# ğŸ“Œ Requirements (From requirements.txt)

Technologies used:  îˆ€fileciteîˆ‚turn2file0îˆ

```
streamlit
pypdf
openai
sentence-transformers
faiss-cpu
langchain
langchain-huggingface
langchain-text-splitters
langchain-community
langchain-openai
python-dotenv
```


