# ğŸ¤– Gen-AI Projects Overview

Welcome to the **Gen-AI Projects** collection! This document provides a quick overview of all projects in this repository. Click on any project title to view its detailed README.

---

## ğŸ“‹ Project Directory

| # | Project | Description |
|---|---------|-------------|
| 1 | [ğŸ§  Local Blog Generator using LLaMA-2](#blog-generation-using-llama) | Create blog drafts instantly using a fully offline AI model â€” simple, private, and requires no API keys. |
| 2 | [ğŸ’¬ Gemini Conversational Chatbot](#conversational-qanda-chatbot-using-gemini-pro) | A conversational AI chatbot built with Google Gemini and Streamlit featuring real-time streaming responses and session-based memory. |
| 3 | [ğŸ“„ PDF Summarizer](#pdf-summarization-tool) | Upload PDFs and receive concise AI-generated summaries using RAG workflow with LangChain, FAISS, and OpenAI. |
| 4 | [ğŸ“š Quiz App - Chat with Documents](#quiz-app-pinecone) | Chat with your PDF documents using a RAG pipeline built on Pinecone, LangChain, and OpenAI. |

---

## ğŸš€ Projects

### ğŸ§  Local Blog Generator using LLaMA-2

**Directory:** `Blog-Generation-Using-LLAMA/`

**[ğŸ“– View Full README](./Blog-Generation-Using%20-LLAMA/README.md)**

#### Overview
A beginner-friendly local blog generator that creates blog drafts instantly without requiring internet or API keys. This project solves common problems with online AI tools by running everything offline on your own device.

#### Key Features
- ğŸ”’ **Full Privacy:** Nothing is sent to the internet
- âš¡ **Fast Generation:** Usually 10-30 seconds on a normal laptop
- ğŸ“ **Customizable:** Choose blog topic, word count, and writing style
- ğŸ¤– **Offline AI:** Uses LLaMA-2 GGML model with CTransformers
- ğŸ“¦ **Auto-Download:** Automatically downloads model from HuggingFace if needed

#### Problem It Solves
- Online AI tools require internet connectivity
- Many require paid API keys
- Sensitive content may be sent to the cloud

#### How It Works
1. User enters blog topic, word count, and writing style
2. App checks if model exists in `models/` folder
3. Model auto-downloads from HuggingFace if needed
4. CTransformers loads the model on your CPU
5. AI generates complete blog locally
6. Blog appears in Streamlit interface

---

### ğŸ’¬ Gemini Conversational Chatbot

**Directory:** `Conversational-QandA-Chatbot-Using-Gemini-Pro/`

**[ğŸ“– View Full README](./Conversational-QandA-Chatbot-Using-Gemini-Pro/README.md)**

#### Overview
A modern conversational AI chatbot built with Google Gemini (gemini-2.5-flash) and Streamlit. It demonstrates professional LLM integration with real-time streaming responses and chat memory management.

#### Key Features
- ğŸŒŠ **Real-time Streaming:** Responses appear chunk-by-chunk for smooth interaction
- ğŸ’¾ **Session Memory:** Maintains running chat history like a real chatbot
- ğŸ¯ **Google Gemini Integration:** Uses cutting-edge gemini-2.5-flash model
- ğŸ¨ **Clean UI:** Lightweight, production-friendly Streamlit interface
- ğŸ“± **Interactive:** Natural language question answering

#### Perfect For
- Demonstrating LLM integration skills
- Rapid prototyping at work
- Building internal chatbots
- Showcasing GenAI expertise in your portfolio

#### Architecture
```
User Question â†’ Streamlit App â†’ GOOGLE_API_KEY (.env) â†’ 
Google Gemini Client â†’ gemini-2.5-flash Model â†’ 
Streaming Response â†’ Session Memory â†’ Updated UI
```

---

### ğŸ“„ PDF Summarizer using LangChain & OpenAI

**Directory:** `PDF-Summarization-Tool/`

**[ğŸ“– View Full README](./PDF-Summarization-Tool/README.md)**

#### Overview
A PDF Summarization application that uses Retrieval-Augmented Generation (RAG) to produce accurate, grounded summaries. Built with Streamlit, LangChain, FAISS, and OpenAI GPT models.

#### Key Features
- ğŸ“¤ **Easy Upload:** Upload any PDF through Streamlit UI
- ğŸ“š **Smart Extraction:** PyPDF extracts text from all pages
- ğŸ”— **LangChain Pipeline:** Professional text splitting and embeddings
- ğŸ—‚ï¸ **FAISS Vector DB:** Semantic search over document content
- ğŸ¯ **Grounded Summaries:** No hallucinationsâ€”summaries based on actual content
- ğŸš€ **GPT-3.5-Turbo:** Uses advanced language model for quality summaries

#### RAG Workflow
```
PDF Upload â†’ Text Extraction â†’ Text Chunking â†’ 
Embeddings Generation (HuggingFace) â†’ FAISS Vector Store â†’ 
Retrieval (Top-5 chunks) â†’ LLM Summarization â†’ 
Final Summary Output
```

#### Tech Stack
- **Streamlit** - User interface
- **PyPDF** - PDF text extraction
- **LangChain** - Text processing and chaining
- **FAISS** - Vector database
- **HuggingFace Embeddings** - `sentence-transformers/all-MiniLM-L6-v2`
- **OpenAI** - `gpt-3.5-turbo-16k` for summarization

---

### ğŸ“š Quiz App - Chat with Your Documents (RAG + Pinecone)

**Directory:** `Quiz-App-Pinecone/`

**[ğŸ“– View Full README](./Quiz-App-Pinecone/README.md)**

#### Overview
An intelligent document Q&A system that lets you chat with PDFs. Users can load a folder of documents, index them with Pinecone, and ask questions that are answered using only the document contentâ€”perfect for building knowledge bases and internal documentation systems.

#### Key Features
- ğŸ“ **Folder Loading:** Load any directory containing PDFs
- ğŸ”„ **Auto Chunking:** Automatically splits PDF pages into overlapping chunks
- ğŸ—‚ï¸ **Pinecone Index:** Creates fresh index for semantic search
- ğŸ” **Semantic Retrieval:** Finds most relevant chunks using vector similarity
- ğŸ’¬ **Chat Interface:** Beautiful conversation history with supporting chunks
- âš™ï¸ **Customizable:** Adjust chunk size, overlap, and retrieval count

#### RAG Pipeline Flow
```
PDF Folder Selection â†’ PDF Loading (PyPDFDirectoryLoader) â†’ 
Text Chunking (RecursiveCharacterTextSplitter) â†’ 
Embeddings Generation (OpenAI) â†’ 
Pinecone Index Creation â†’ 
Document Vector Upsert â†’ 
User Question â†’ 
Semantic Retrieval (Top-K) â†’ 
Prompt Construction â†’ 
OpenAI Response Generation â†’ 
Answer with Context
```

#### Tech Stack
- **Pinecone** - Vector database for semantic search
- **LangChain** - Document processing and retrieval
- **OpenAI** - Embeddings and LLM
- **Streamlit** - User interface
- **PyPDF** - PDF document loading

---

## ğŸ› ï¸ Getting Started

Each project has its own directory with:
- `app.py` - Main application file
- `requirements.txt` - Python dependencies
- `README.md` - Detailed documentation

To get started with any project:

1. Navigate to the project directory
2. Read the project's README file (click the link above)
3. Install dependencies: `pip install -r requirements.txt`
4. Set up required API keys (if needed)
5. Run the app: `streamlit run app.py`

---

## ğŸ“ Notes

- All projects use **Streamlit** for the user interface
- Most projects integrate with **OpenAI** or **Google APIs**
- Each project demonstrates different GenAI patterns (generation, conversation, retrieval, RAG)
- See individual README files for detailed setup instructions

---

**Last Updated:** December 2025

For questions or improvements, refer to the individual project README files.
